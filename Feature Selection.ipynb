{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# 3. Feature selection",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Feature selection is a process of selecting a subset of relevant features (or attributes) from the original set of features in a dataset. The goal of feature selection is to choose the most relevant and important features, thereby reducing dimensionality, and improving model performance.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### 3.1 SelectKBest (Filter Method)\n\nselectKBest is a filter-based method that evaluates each feature based on a statistical test (in this case, f_classif) and selects the top k features that have the highest correlation with the target variable.\nIn our case, we selected the top 7 features.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n# Load the dataset\ndfs = pd.read_csv('Housing_cleaned.csv')\n\n# List of non-numeric columns that need manual conversion\nnon_numeric_columns = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'furnishingstatus', 'prefarea']\n\n# Convert non-numeric columns to numeric using LabelEncoder\nlabel_encoder = LabelEncoder()\nfor col in non_numeric_columns:\n    dfs[col] = label_encoder.fit_transform(data[col])\n\n# X includes all columns except the target column 'price'\nX = dfs.iloc[:, 1:]  # All features\ny = (dfs['price'] > dfs['price'].median()).astype(int)  # Binary classification (1 if price increases, 0 otherwise)\n\n# Select the top 7 features using SelectKBest\nselector = SelectKBest(score_func=f_classif, k=7)\nX_new = selector.fit_transform(X, y)\n\n# Print the selected features\nselected_features = X.columns[selector.get_support()]\nprint(\"Selected Features (SelectKBest):\", selected_features)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Selected Features (SelectKBest): Index(['area', 'bedrooms', 'bathrooms', 'stories', 'mainroad',\n       'airconditioning', 'prefarea'],\n      dtype='object')\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "source": "### 3.2 Variance Threshold (Filter Method)\n\nThis method removes all features whose variance doesn’t meet the given threshold. Low-variance features are considered less informative for predicting the target.\nwe set the variance threshold to 0.2, meaning features with low variability (less distinct information) are removed.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.feature_selection import VarianceThreshold\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n# Load the dataset\ndfs = pd.read_csv('Housing_cleaned.csv')\n\n# List of non-numeric columns that need manual conversion\nnon_numeric_columns = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'furnishingstatus', 'prefarea']\n\n# Convert non-numeric columns to numeric using LabelEncoder\nlabel_encoder = LabelEncoder()\nfor col in non_numeric_columns:\n    dfs[col] = label_encoder.fit_transform(data[col])\n\n# X includes all columns except the target column 'price'\nX = dfs.iloc[:, 1:]  # All features\ny = (dfs['price'] > dfs['price'].median()).astype(int)  # Binary classification (1 if price increases, 0 otherwise)\n\n# Apply Variance Threshold\nselector = VarianceThreshold(threshold=0.2)  # Keep all features with non-zero variance\nX_new = selector.fit_transform(X)\n\n# Print the selected features\nselected_features = X.columns[selector.get_support()]\nprint(\"Selected Features (Variance Threshold):\", selected_features)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Selected Features (Variance Threshold): Index(['area', 'bedrooms', 'bathrooms', 'stories', 'basement',\n       'airconditioning', 'parking', 'furnishingstatus'],\n      dtype='object')\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "source": "### 3.3 Recursive Feature Elimination (RFE)\n\n\nRFE is a wrapper method that iteratively removes the least important features, based on a model’s performance, to select the best subset of features.\nWe used Logistic Regression with 2000 iterations as the underlying model and selected the top 7 features.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n# Load the dataset\ndfs = pd.read_csv('Housing_cleaned.csv')\n\n# List of non-numeric columns that need manual conversion\nnon_numeric_columns = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'furnishingstatus', 'prefarea']\n\n# Convert non-numeric columns to numeric using LabelEncoder\nlabel_encoder = LabelEncoder()\nfor col in non_numeric_columns:\n    dfs[col] = label_encoder.fit_transform(dfs[col])\n\n# X includes all columns except the target column 'price'\nX = dfs.iloc[:, 1:]  # All features\ny = (dfs['price'] > dfs['price'].median()).astype(int)  # Binary classification (1 if price increases, 0 otherwise)\n\n# Use RFE with Logistic Regression as the underlying model\nmodel = LogisticRegression(max_iter=2000)\nrfe = RFE(model, n_features_to_select=7)  # Select top 7 features\nX_new = rfe.fit_transform(X, y)\n\n# Print the selected features\nselected_features = X.columns[rfe.get_support()]\nprint(\"Selected Features (RFE):\", selected_features)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Selected Features (RFE): Index(['bathrooms', 'stories', 'mainroad', 'guestroom', 'basement',\n       'airconditioning', 'prefarea'],\n      dtype='object')\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "markdown",
      "source": "### 3.4   L1 Regularization (Lasso - Embedded Method)\n\nLasso (Least Absolute Shrinkage and Selection Operator) uses L1 regularization, which adds a penalty for large coefficients in the model, causing some to shrink to zero.\nFeatures with zero coefficients are removed, while those with non-zero coefficients are retained.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.linear_model import Lasso\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n# Load the dataset\ndfs = pd.read_csv('Housing_cleaned.csv')\n\n# List of non-numeric columns that need manual conversion\nnon_numeric_columns = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'furnishingstatus', 'prefarea']\n\n# Convert non-numeric columns to numeric using LabelEncoder\nlabel_encoder = LabelEncoder()\nfor col in non_numeric_columns:\n    dfs[col] = label_encoder.fit_transform(data[col])\n\n# X includes all columns except the target column 'price'\nX = dfs.iloc[:, 1:]  # All features\ny = (dfs['price'] > dfs['price'].median()).astype(int)  # Binary classification (1 if price increases, 0 otherwise)\n\n# Apply L1 Regularization (Lasso)\nmodel = Lasso(alpha=0.1, max_iter=1000)\nmodel.fit(X, y)\n\n# Print the selected features based on non-zero coefficients\nselected_features = X.columns[model.coef_ != 0]\nprint(\"Selected Features (Lasso):\", selected_features)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Selected Features (Lasso): Index(['area', 'stories'], dtype='object')\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "source": "### 3.5 Hybrid Method (SelectKBest + RFE)\n\nThis is a combination of both the filter (SelectKBest) and wrapper (RFE) methods. First, SelectKBest reduces the feature space by selecting the top 10 features. Then, RFE further reduces the feature set by selecting the best 7 features based on model performance.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nfrom sklearn.feature_selection import SelectKBest, f_classif, RFE\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n# Load the dataset\ndfs = pd.read_csv('Housing_cleaned.csv')\n\n# List of non-numeric columns that need manual conversion\nnon_numeric_columns = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'furnishingstatus', 'prefarea']\n\n# Convert non-numeric columns to numeric using LabelEncoder\nlabel_encoder = LabelEncoder()\nfor col in non_numeric_columns:\n    dfs[col] = label_encoder.fit_transform(dfs[col])\n\n# X includes all columns except the target column 'price'\nX = dfs.iloc[:, 1:]  # All features\ny = (dfs['price'] > dfs['price'].median()).astype(int)  # Binary classification (1 if price increases, 0 otherwise)\n\n# Step 1: Use SelectKBest (Filter Method) to reduce feature space\nselector = SelectKBest(score_func=f_classif, k=10)  # Select the top 10 features\nX_filtered = selector.fit_transform(X, y)\n\n# Step 2: Use RFE (Wrapper Method) to find the best candidate subset\nmodel = LogisticRegression(max_iter=1000)\nrfe = RFE(model, n_features_to_select=7)  # Further reduce to 7 features\nX_new = rfe.fit_transform(X_filtered, y)\n\n# Print the selected features\nselected_features_kbest = X.columns[selector.get_support()]\nselected_features_rfe = selected_features_kbest[rfe.get_support()]\nprint(\"Selected Features after Hybrid FS (SelectKBest + RFE):\", selected_features_rfe)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Selected Features after Hybrid FS (SelectKBest + RFE): Index(['bedrooms', 'bathrooms', 'stories', 'mainroad', 'guestroom',\n       'airconditioning', 'prefarea'],\n      dtype='object')\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 16
    },
    {
      "cell_type": "markdown",
      "source": "Best Method for Our Data\nBest Method:\n\nHybrid Method (SelectKBest + RFE)\nReason:\n\nThe Hybrid Method provides a balance between feature relevance and interaction. SelectKBest reduces the number of irrelevant features based on their correlation with the target, while RFE ensures that the selected features are optimized for model performance.\nThe selected features using the hybrid method combine the strengths of both techniques, offering the best compromise between simplicity and accuracy for this dataset. This approach provides a robust feature set that captures both statistical significance and model-based importance.",
      "metadata": {}
    }
  ]
}
